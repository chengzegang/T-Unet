{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NErqeKmG_BgH"
   },
   "outputs": [],
   "source": [
    "from os import path, listdir, mkdir\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal as sps\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pM4QGf9r2_A0",
    "outputId": "51f16ce1-7a10-46f1-94ee-fbd333f8e748"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tslRQYgjFtem",
    "outputId": "ff8a16af-5d5a-45bc-cea5-f27406c4303c"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7cOHWFC3Dob"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_wav_mono(filename, out_rate):\n",
    "    rate, data = wavfile.read(filename)\n",
    "    data = sps.resample(data, int(out_rate / rate * len(data)))\n",
    "    data = data[:, np.newaxis]\n",
    "    data = np.stack(np.array_split(data, 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WSpWr-sK82g"
   },
   "outputs": [],
   "source": [
    "def unet(pretrained_weights = None,input_size = (None, 1)):\n",
    "  inputs = Input(input_size)\n",
    "  norm0 = tf.keras.layers.BatchNormalization()(inputs)\n",
    "\n",
    "  cnn1 = Conv1D(16, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(norm0)\n",
    "  cnn1 = Conv1D(16, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(cnn1)\n",
    "  cnn1 = Conv1D(16, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(cnn1)\n",
    "  pool1 = MaxPooling1D(pool_size=10)(cnn1)\n",
    "  norm1 = BatchNormalization()(pool1)\n",
    "\n",
    "  cnn2 = Conv1D(32, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(norm1)\n",
    "  cnn2 = Conv1D(32, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(cnn2)\n",
    "  cnn2 = Conv1D(32, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(cnn2)\n",
    "  pool2 = MaxPooling1D(pool_size=10)(cnn2)\n",
    "  norm2 = BatchNormalization()(pool2)\n",
    "\n",
    "  cnn3 = Conv1D(64, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(norm2)\n",
    "  cnn3 = Conv1D(64, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(cnn3)\n",
    "  cnn3 = Conv1D(64, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(cnn3)\n",
    "  drop3 = Dropout(0.5)(cnn3)\n",
    "  pool3 = MaxPooling1D(pool_size=10)(drop3)\n",
    "  norm3 = BatchNormalization()(pool3)\n",
    "  \n",
    "  cnn4 = Conv1D(128, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(norm3)\n",
    "  cnn4 = Conv1D(128, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(cnn4)\n",
    "  cnn4 = Conv1D(128, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(cnn4)\n",
    "  drop4 = Dropout(0.5)(cnn4)\n",
    "  norm4 = BatchNormalization()(drop4)\n",
    "\n",
    "  up5 = UpSampling1D(10)(norm4) \n",
    "  cnn5 = Conv1D(64, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(up5)\n",
    "  concat5 = Concatenate()([drop3, cnn5])\n",
    "  cnn5 = Conv1D(64, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(concat5)\n",
    "  cnn5 = Conv1D(64, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(cnn5)\n",
    "  norm5 = BatchNormalization()(cnn5)\n",
    "\n",
    "  up6 = UpSampling1D(10)(norm5) \n",
    "  cnn6 = Conv1D(32, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(up6)\n",
    "  concat6 = Concatenate()([cnn2, cnn6])\n",
    "  cnn6 = Conv1D(32, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(concat6)\n",
    "  cnn6 = Conv1D(32, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(cnn6)\n",
    "  norm6 = BatchNormalization()(cnn6)\n",
    "\n",
    "  up7 = UpSampling1D(10)(norm6) \n",
    "  cnn7 = Conv1D(16, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(up7)\n",
    "  concat7 = Concatenate()([cnn1, cnn7])\n",
    "  cnn7 = Conv1D(16, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(concat7)\n",
    "  cnn7 = Conv1D(16, 100, activation = 'relu', padding='same', kernel_initializer='glorot_uniform')(cnn7)\n",
    "  norm7 = BatchNormalization()(cnn7)\n",
    "\n",
    "  cnn8 = Conv1D(8, 100, activation = 'linear', padding='same', kernel_initializer='glorot_uniform')(norm7)\n",
    "  cnn8 = Conv1D(1, 100, activation = 'linear', padding='same', kernel_initializer='glorot_uniform')(cnn8)\n",
    "\n",
    "  model = Model(inputs = inputs, outputs = cnn8)\n",
    "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                steps_per_execution = 1000,\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "  \n",
    "  if(pretrained_weights):\n",
    "    model.load_weights(pretrained_weights)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHFQDQulEIRr"
   },
   "outputs": [],
   "source": [
    "with tpu_strategy.scope():\n",
    "  model = unet()\n",
    "\n",
    "@tf.function\n",
    "def load_a_batch_data(file_src_list):\n",
    "  size = len(file_src_list)\n",
    "  X = [0] * size\n",
    "  Y = [0] * size\n",
    "  for i in range(size):\n",
    "    x = load_wav_mono(file_src_list[i][0], 8000)\n",
    "    y = load_wav_mono(file_src_list[i][1], 8000)\n",
    "    X[i] = x\n",
    "    Y[i] = y\n",
    "  X = tf.concat(X, 0)\n",
    "  Y = tf.concat(Y, 0)\n",
    "  return X, Y\n",
    "\n",
    "@tf.function\n",
    "def load_batch_data(load_batch_size, start_point, random_select=False):\n",
    "  mixed_srcs = listdir('/content/drive/MyDrive/songs/mix/')\n",
    "  mixed_srcs.sort()\n",
    "  talk_srcs = listdir('/content/drive/MyDrive/songs/vocal/')\n",
    "  talk_srcs.sort()\n",
    "  mixed_srcs = ['/content/drive/MyDrive/songs/mix/' + el for el in mixed_srcs]\n",
    "  talk_srcs = ['/content/drive/MyDrive/songs/vocal/' + el for el in talk_srcs]\n",
    "  \n",
    "  data_total_size = len(mixed_srcs)\n",
    "\n",
    "  data_srcs = list(zip(mixed_srcs, talk_srcs))\n",
    "  if random_select:\n",
    "    data_srcs = random.sample(data_srcs, load_batch_size)\n",
    "  else:\n",
    "    start = start_point\n",
    "    end = data_total_size if start_point + load_batch_size >= data_total_size else start_point + load_batch_size\n",
    "\n",
    "    data_srcs = data_srcs[start:end] \n",
    "  \n",
    "\n",
    "  X, Y = load_a_batch_data(data_srcs)\n",
    "  return X, Y\n",
    "\n",
    "def train_model(model, data_size, batch_size, epoches, start_point):\n",
    "  LOAD_BATCH_SIZE = 200\n",
    "  t = round(data_size / LOAD_BATCH_SIZE)\n",
    "  checkpoint_filepath = '/content/drive/MyDrive/checkpoint/checkpoint.h5'\n",
    "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      save_weights_only=True,\n",
    "      monitor='val_loss',\n",
    "      mode='min',\n",
    "      save_best_only=True\n",
    "      )\n",
    "  total_time = 0\n",
    "  for i in range(t):\n",
    "    t_start = time.time()\n",
    "    print('Loading Data Batch ' + str(i + 1) + '/' + str(t))\n",
    "    X, Y = load_batch_data(LOAD_BATCH_SIZE, start_point, False)\n",
    "    print('Data Batch ' + str(i + 1) + '/' + str(t) + ' has been loaded, ready to train. ')\n",
    "    start_point += LOAD_BATCH_SIZE\n",
    "    model.fit(x=X, y=Y, batch_size=batch_size, epochs = epoches, validation_split=0.2, callbacks=[model_checkpoint_callback], shuffle=True)\n",
    "    model.save('/content/drive/MyDrive/model/model.h5')\n",
    "    t_end = time.time()\n",
    "    t_per_loop = t_end - t_start\n",
    "    total_time += t_per_loop\n",
    "    print('Time per Loop: ' + str(round(t_per_loop)))\n",
    "  print('Total Time: ' + str(round(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3JV9jMo51hJ",
    "outputId": "f141ec05-195e-4bd4-a24f-0913774647b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 1)      4           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 16)     1616        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 16)     25616       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 16)     25616       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, None, 16)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 16)     64          max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 32)     51232       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 32)     102432      conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 32)     102432      conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, None, 32)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 32)     128         max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     204864      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     409664      conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 64)     409664      conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 64)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, None, 64)     0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 64)     256         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, None, 128)    819328      batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, None, 128)    1638528     conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 128)    1638528     conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 128)    0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, 128)    512         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d (UpSampling1D)    (None, None, 128)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 64)     819264      up_sampling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 128)    0           dropout[0][0]                    \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 64)     819264      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 64)     409664      conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1D)  (None, None, 64)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 32)     204832      up_sampling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 64)     0           conv1d_5[0][0]                   \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 32)     204832      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 32)     102432      conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, 32)     128         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_2 (UpSampling1D)  (None, None, 32)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 16)     51216       up_sampling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 32)     0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 16)     51216       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 16)     25616       conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, 16)     64          conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, None, 8)      12808       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, None, 1)      801         conv1d_21[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,132,877\n",
      "Trainable params: 8,132,171\n",
      "Non-trainable params: 706\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fd2lMPVzHCCI",
    "outputId": "74b9b6d2-9d8d-4e17-e597-450433412c4e"
   },
   "outputs": [],
   "source": [
    "train_model(model, 2000, 32, 300, 6000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "encoder_decoder.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
